{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8972089c",
   "metadata": {},
   "source": [
    "## Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822b592",
   "metadata": {},
   "source": [
    "$$a_T(x)= \\sum_{t=1}^T \\alpha_t b_t(x), \\quad x\\in X, \\quad b_t:X\\rightarrow \\mathbb{R}, \\quad\\alpha_t\\in \\mathbb{R}_+$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731776c",
   "metadata": {},
   "source": [
    "**Эвристика:** обучаем $\\alpha_t, b_t$ при фиксированных прилылущих.\n",
    "\n",
    "**Критерий качества** с заданной гладкой функцией потерь $\\mathcal{L}(b,y)$:\n",
    "$$Q(\\alpha;b;X^l)=\\sum_{i=1}^l\\mathcal{L}(\\sum_{t=1}^{T-1}\\alpha_t b_t(x_i)+\\alpha b(x_i), y_i)\\rightarrow\\underset{\\alpha, b}{min}$$\n",
    "- $(a_{T-1,i})_{i=1}^l$ - вектор текущего приближения\n",
    "- $(a_{T,i})_{i=1}^l$ - вектор следующего приближения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7e584",
   "metadata": {},
   "source": [
    "Градиентный метод минимизации $Q(F)\\rightarrow min, f\\in \\mathbb{R}^l$:\n",
    "- $a_{0,i}:=$ начальное приближение;\n",
    "- $a_{T,i}:= a_{T-1,i}-\\lambda g_i$ начальное приближение, $i=1,...,l$, где $g_i=\\mathcal{L}_f'(a_{T-1,i}, y_i)$ - компоненты вектора градиента, $\\lambda$- градиентный шаг;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a06f8",
   "metadata": {},
   "source": [
    "А добавление одного базового алгоритма: \n",
    "$a_{T,i}:= a_{T-1,i}-\\lambda b(x_i), \\quad i=1,...,l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53feb9c",
   "metadata": {},
   "source": [
    "Тогда будем искать базовый алгоритм, который приближает вектор антиградиента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b0b25",
   "metadata": {},
   "source": [
    "**Алгоритм градиентного бустинга:**\n",
    "\n",
    "*Вход:* \n",
    "\n",
    "- $X^l, Y^l$ - обучающая выборка;\n",
    "- $T$ - максимальное число базовых алгоритмов;\n",
    "- $\\lambda$ - скорость обучения.\n",
    "\n",
    "*Выход:*\n",
    "- базовые алгоритмы и их веса $\\alpha_tb_t, \\quad t=1,...,T$\n",
    "\n",
    "*Алгоритм:*\n",
    "1. инициализируем вектор начальный значений: $a_i=1/l, \\quad i=1,..l;$\n",
    "2. для всех $t=1,...,T:$\n",
    "- Обучить базовый алгоритм, приближающий антиградиент:  \n",
    "$b_t:=arg\\underset{b\\in B}{min}\\sum_{i=1}^l(b(x_i)+\\mathcal{L}'(w_i,y_i))^2$\n",
    "- найти вес базового алгоритма $\\alpha_t$:  \n",
    "$\\alpha_t:=arg\\underset{a>0}{min}\\sum_{i=1}^l\\mathcal{L}(a_i+\\lambda b_t(x_i);y_i)$\n",
    "- обновить значения:  \n",
    "$a_i:=a_i+\\alpha_t b_t(x_i); \\quad i=1,...,l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee533d4",
   "metadata": {},
   "source": [
    "**Алгоритм градиентного бустинга адаптация под логарифмическую функцию потерь:**\n",
    "$$Q(\\alpha;b;X^l) = \\sum_{i=1}^l log_2(1+e^{-y_i*b(x_i) })\\rightarrow \\underset{w}{min}$$\n",
    "Антиградиент: \n",
    "\n",
    "$\\mathcal{L}'(b(x_i),y_i))= y_i*\\sigma(-b(x_i)*y_i)$, где $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "*Вход:* \n",
    "\n",
    "- $X^l, Y^l$ - обучающая выборка;\n",
    "- $T$ - максимальное число базовых алгоритмов;\n",
    "- $\\lambda$ - скорость обучения.\n",
    "\n",
    "*Выход:*\n",
    "- базовые алгоритмы и их веса $\\alpha_tb_t, \\quad t=1,...,T$\n",
    "- в качестве базовой функции возьмем DecisionTreeRegressor()\n",
    "\n",
    "*Алгоритм:*\n",
    "1. инициализируем вектор начальный значений: $a_i=1/l, \\quad i=1,..l;$\n",
    "2. для всех $t=1,...,T:$\n",
    "- Обучить базовый алгоритм на наблюдениях $\\mathcal{L}'(b(x_i),y_i))$;  \n",
    "- Обновить значения: \n",
    "$a_i:=a_i+\\lambda b_t(x_i); \\quad i=1,...,l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca175f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daca4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]\n",
    "\n",
    "data_moon = datasets[1]\n",
    "\n",
    "X, y = data_moon\n",
    "\n",
    "y = np.array([ -1.0 if i == 0 else float(i) for i in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb8320",
   "metadata": {},
   "source": [
    "## Задание "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3d682",
   "metadata": {},
   "source": [
    "1. Реализовать алгоритм градиентного бустинга для логарифмической функции потерь.\n",
    "2. Оценить качество работы алгоритма на сгенерированных трех наборах данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99a5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoida(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabb9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_boost(X, y, T, lambda_):\n",
    "    l = X.shape[0]\n",
    "    a = np.ones(l) / l\n",
    "    b = []\n",
    "    y[y == 0] = -1\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        # Вычисляем значение антиградиента\n",
    "        L = y * sigmoida(- a * y)\n",
    "        \n",
    "        model = DecisionTreeRegressor()\n",
    "        \n",
    "        model.fit(X, L)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        a += lambda_ * y_pred\n",
    "        \n",
    "        b.append(model)\n",
    "        \n",
    "    return np.sign(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e6b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: 0.0\n",
      "Dataset 2: 0.0\n",
      "Dataset 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(datasets):\n",
    "    a_sign = grad_boost(X, y, 50, 0.1)\n",
    "    N = np.sum(a_sign[a_sign != y])\n",
    "    \n",
    "    print(f'Dataset {i + 1}: {N}')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
